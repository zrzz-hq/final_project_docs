{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Faster R-CNN for Handwritten Digit Detection This project focuses on developing a robust solution for handwritten digit detection using the Faster R-CNN framework. Handwritten digit detection is critical in applications like automated form processing, educational assessments, and financial record management. This project demonstrates how Faster R-CNN can be adapted to tackle the complexities of real-world handwritten digit detection tasks. Project Objectives Develop a Detection Model : Adapt Faster R-CNN for detecting multiple handwritten digits in a single image. Handle Real-world Challenges : Address overlapping digits, diverse writing styles, and varying object scales. Analyze Backbone Performance : Evaluate the impact of different backbone architectures, including ResNet-34-FPN, ResNet-50-FPN, and MobileNetV3-FPN. Methods Dataset Preparation : Collected a dataset of 4190 RGB images containing handwritten digits with diverse writing styles. Preprocessed images to ensure uniform size (640x640 pixels) and annotated bounding boxes. Model Adaptation : Modified the Faster R-CNN framework to classify digits (0-9) and background. Experimented with different backbones for feature extraction, such as ResNet and MobileNetV3. Training Setup : Leveraged transfer learning using pre-trained weights on the COCO dataset. Employed a Step Decay learning rate scheduler to stabilize training. Evaluation Metrics : Evaluated the model using mean Average Precision (mAP), mAP@50, mAP@75, and class-specific metrics. Results Achieved a mean Average Precision (mAP) of 52.2% on the test set using ResNet-34 as the backbone. MobileNetV3 demonstrated strong performance in detecting small and large objects. Fine-tuning the backbone parameters significantly improved detection accuracy for lightweight architectures. Project Layout weights/ # This is where pretrained models are stored trains/ # Contains run folders for each training process utils.py train.py # The train module test.py # The test module environment.yml # Defines conda environment for the project requirements.txt Note that the weights and trains folder will be created automatically if needed","title":"Faster R-CNN for Handwritten Digit Detection"},{"location":"#faster-r-cnn-for-handwritten-digit-detection","text":"This project focuses on developing a robust solution for handwritten digit detection using the Faster R-CNN framework. Handwritten digit detection is critical in applications like automated form processing, educational assessments, and financial record management. This project demonstrates how Faster R-CNN can be adapted to tackle the complexities of real-world handwritten digit detection tasks.","title":"Faster R-CNN for Handwritten Digit Detection"},{"location":"#project-objectives","text":"Develop a Detection Model : Adapt Faster R-CNN for detecting multiple handwritten digits in a single image. Handle Real-world Challenges : Address overlapping digits, diverse writing styles, and varying object scales. Analyze Backbone Performance : Evaluate the impact of different backbone architectures, including ResNet-34-FPN, ResNet-50-FPN, and MobileNetV3-FPN.","title":"Project Objectives"},{"location":"#methods","text":"Dataset Preparation : Collected a dataset of 4190 RGB images containing handwritten digits with diverse writing styles. Preprocessed images to ensure uniform size (640x640 pixels) and annotated bounding boxes. Model Adaptation : Modified the Faster R-CNN framework to classify digits (0-9) and background. Experimented with different backbones for feature extraction, such as ResNet and MobileNetV3. Training Setup : Leveraged transfer learning using pre-trained weights on the COCO dataset. Employed a Step Decay learning rate scheduler to stabilize training. Evaluation Metrics : Evaluated the model using mean Average Precision (mAP), mAP@50, mAP@75, and class-specific metrics.","title":"Methods"},{"location":"#results","text":"Achieved a mean Average Precision (mAP) of 52.2% on the test set using ResNet-34 as the backbone. MobileNetV3 demonstrated strong performance in detecting small and large objects. Fine-tuning the backbone parameters significantly improved detection accuracy for lightweight architectures.","title":"Results"},{"location":"#project-layout","text":"weights/ # This is where pretrained models are stored trains/ # Contains run folders for each training process utils.py train.py # The train module test.py # The test module environment.yml # Defines conda environment for the project requirements.txt Note that the weights and trains folder will be created automatically if needed","title":"Project Layout"},{"location":"getting_start/HiPerGator/","text":"Getting Start on HiPerGator This guide provides instructions for deploying the project on HiPerGator, supporting both Jupyter Notebook and Command Line workflows. Prerequisites Access to HiPerGator with a valid account. Deployment using Jupyter Notebook (Recommend) Start a new jupyter session from here on HiPerGator and connect to it. Open a terminal tab, then clone the repo by: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git Create a jupyter notebook file with kernel PyTorch-2.2.0 in the final-project-code-report-lonely-life folder. In the created jupyter notebook, paste the following code into a cell and run the cell. !pip install torchmetrics torchmetrics[detection] Deployment using Command Line Create a new terminal session from here on HiPerGator and connect to it. Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Load Required Modules: module load pytorch/2.2.0 Install Dependencies: pip install torchmetrics torchmetrics[detection]","title":"On HiPerGator"},{"location":"getting_start/HiPerGator/#getting-start-on-hipergator","text":"This guide provides instructions for deploying the project on HiPerGator, supporting both Jupyter Notebook and Command Line workflows.","title":"Getting Start on HiPerGator"},{"location":"getting_start/HiPerGator/#prerequisites","text":"Access to HiPerGator with a valid account.","title":"Prerequisites"},{"location":"getting_start/HiPerGator/#deployment-using-jupyter-notebook-recommend","text":"Start a new jupyter session from here on HiPerGator and connect to it. Open a terminal tab, then clone the repo by: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git Create a jupyter notebook file with kernel PyTorch-2.2.0 in the final-project-code-report-lonely-life folder. In the created jupyter notebook, paste the following code into a cell and run the cell. !pip install torchmetrics torchmetrics[detection]","title":"Deployment using Jupyter Notebook (Recommend)"},{"location":"getting_start/HiPerGator/#deployment-using-command-line","text":"Create a new terminal session from here on HiPerGator and connect to it. Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Load Required Modules: module load pytorch/2.2.0 Install Dependencies: pip install torchmetrics torchmetrics[detection]","title":"Deployment using Command Line"},{"location":"getting_start/your_own_computer/","text":"Getting Start on Your Own Computer This guide provides instructions for deploying the project locally, using either Conda or pip . Prerequisites Python 3.9+ installed on your system. GPU support (optional but recommended). Deployment using Conda (Recommend) Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Create and Activate Conda Environment: conda env create -f environment.yml conda activate fasterrcnn-env Deployment using pip Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Install Dependencies: pip install -r requirements.txt","title":"On your own computer"},{"location":"getting_start/your_own_computer/#getting-start-on-your-own-computer","text":"This guide provides instructions for deploying the project locally, using either Conda or pip .","title":"Getting Start on Your Own Computer"},{"location":"getting_start/your_own_computer/#prerequisites","text":"Python 3.9+ installed on your system. GPU support (optional but recommended).","title":"Prerequisites"},{"location":"getting_start/your_own_computer/#deployment-using-conda-recommend","text":"Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Create and Activate Conda Environment: conda env create -f environment.yml conda activate fasterrcnn-env","title":"Deployment using Conda (Recommend)"},{"location":"getting_start/your_own_computer/#deployment-using-pip","text":"Clone the Repository: git clone https://github.com/UF-EEL5840-EEE4773-Fall-2024/final-project-code-report-lonely-life.git cd final-project-code-report-lonely-life Install Dependencies: pip install -r requirements.txt","title":"Deployment using pip"},{"location":"test/run/","text":"test.run TODO","title":"test.run"},{"location":"test/run/#testrun","text":"TODO","title":"test.run"},{"location":"test/test/","text":"test.test test.test ( image_dir, label_dir, result_dir = None , weights = 'Pretrained' , backbone = 'resnet34_fpn' , device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") , batch_size= 32 , conf_threshold = 0.8 ) \u2192 tuple[dict, float] The function evaluates a pre-trained Faster R-CNN model on a given test dataset and calculates performance metrics, including Mean Average Precision (mAP) and Intersection over Union (IoU). It also optionally saves visualized results with bounding boxes drawn on the test images. Arguments image_dir ( str ): Path to the directory containing test images. label_dir ( str ): Path to the directory containing test labels in YOLO format. result_dir ( str , optional): Directory to save test results with bounding boxes. If None , results are not saved. weights ( str ): Specifies the model weights to use: Pretrained : Loads pre-trained weights from a source. Recent : Loads weights from best.pt in the most recent run folder. Default : Uses original weights of Faster R-CNN. None : Starts with randomly initialized weights. Custom path: Specify the path to a .pt file containing saved weights. backbone ( str , optional if weights is set a custom path): Backbone architecture. Options: resnet34_fpn resnet50_fpn mobilenetv3_fpn device ( torch.device ): Device to run evaluation (CPU or GPU). batch_size ( int ): Number of samples processed per batch. conf_threshold ( float ): Confidence threshold for filtering predictions. Returns mAP ( dict ): Mean Average Precision for the test dataset. IoU ( float ): Intersection over Union metric for the dataset. How It Works Loads the test images and labels using a custom dataset class ( YoloDataset ). Initializes a Faster R-CNN model with the specified backbone and weights. Runs the model on the test dataset in batches, filtering predictions based on confidence. Updates metrics ( mAP , IoU ) for each batch. Optionally, saves the test results with bounding boxes drawn on the images to the specified directory. Example Usage from test import test from pprint import pprint mAP, IoU = test( image_dir='./data/test_set/images', label_dir='./data/test_set/labels', result_dir='./results', weights='Pretrained', backbone='resnet34_fpn', batch_size=16, conf_threshold=0.8 ) pprint(\"Mean Average Precision:\", mAP) pprint(\"Intersection over Union:\", IoU) Python Output: Testing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:10<00:00, 2.62it/s] {'map': 0.5220838189125061, 'map_50': 0.9335460662841797, 'map_75': 0.5252417922019958, 'map_large': 0.5681636929512024, 'map_medium': 0.4912945032119751, 'map_small': 0.4415474832057953} 0.6990163922309875 Images with predicted bounding boxes are saved in ./results .","title":"test.test"},{"location":"test/test/#testtest","text":"test.test ( image_dir, label_dir, result_dir = None , weights = 'Pretrained' , backbone = 'resnet34_fpn' , device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") , batch_size= 32 , conf_threshold = 0.8 ) \u2192 tuple[dict, float] The function evaluates a pre-trained Faster R-CNN model on a given test dataset and calculates performance metrics, including Mean Average Precision (mAP) and Intersection over Union (IoU). It also optionally saves visualized results with bounding boxes drawn on the test images.","title":"test.test"},{"location":"test/test/#arguments","text":"image_dir ( str ): Path to the directory containing test images. label_dir ( str ): Path to the directory containing test labels in YOLO format. result_dir ( str , optional): Directory to save test results with bounding boxes. If None , results are not saved. weights ( str ): Specifies the model weights to use: Pretrained : Loads pre-trained weights from a source. Recent : Loads weights from best.pt in the most recent run folder. Default : Uses original weights of Faster R-CNN. None : Starts with randomly initialized weights. Custom path: Specify the path to a .pt file containing saved weights. backbone ( str , optional if weights is set a custom path): Backbone architecture. Options: resnet34_fpn resnet50_fpn mobilenetv3_fpn device ( torch.device ): Device to run evaluation (CPU or GPU). batch_size ( int ): Number of samples processed per batch. conf_threshold ( float ): Confidence threshold for filtering predictions.","title":"Arguments"},{"location":"test/test/#returns","text":"mAP ( dict ): Mean Average Precision for the test dataset. IoU ( float ): Intersection over Union metric for the dataset.","title":"Returns"},{"location":"test/test/#how-it-works","text":"Loads the test images and labels using a custom dataset class ( YoloDataset ). Initializes a Faster R-CNN model with the specified backbone and weights. Runs the model on the test dataset in batches, filtering predictions based on confidence. Updates metrics ( mAP , IoU ) for each batch. Optionally, saves the test results with bounding boxes drawn on the images to the specified directory.","title":"How It Works"},{"location":"test/test/#example-usage","text":"from test import test from pprint import pprint mAP, IoU = test( image_dir='./data/test_set/images', label_dir='./data/test_set/labels', result_dir='./results', weights='Pretrained', backbone='resnet34_fpn', batch_size=16, conf_threshold=0.8 ) pprint(\"Mean Average Precision:\", mAP) pprint(\"Intersection over Union:\", IoU) Python Output: Testing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:10<00:00, 2.62it/s] {'map': 0.5220838189125061, 'map_50': 0.9335460662841797, 'map_75': 0.5252417922019958, 'map_large': 0.5681636929512024, 'map_medium': 0.4912945032119751, 'map_small': 0.4415474832057953} 0.6990163922309875 Images with predicted bounding boxes are saved in ./results .","title":"Example Usage"},{"location":"train/train/","text":"train.train train.train ( image_dir, label_dir, weights = 'Default' , backbone = 'resnet50_fpn' , device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") , learning_rate = 1e-4 , batch_size = 32 , n_epochs = 50 , backbone_frozen = False ) \u2192 tuple[tuple[float],tuple[float]] The function trains a Faster R-CNN model on a specified dataset and returns the training losses and validation mAP for each epoch. It creates a runXX folder under the trains directory, where XX is a number denoting the current training session's index. This folder contains the log file and the best model, selected based on the highest validation mAP score. Arguments image_dir ( str ): Path to the directory containing training images. label_dir ( str ): Path to the directory containing training labels in YOLO format. weights ( str ): Specifies how the model is initialized. Options: Pretrained : Uses pre-trained weights on our dataset. Recent : Uses the weights in the most recent run folder. Default : Uses original weights of Faster R-CNN. None : Starts with random weights. Custom Path: A .pt file containing saved weights. backbone ( str ): Backbone architecture. Options: resnet34_fpn resnet50_fpn mobilenetv3_fpn device ( torch.device ): Device to run training (CPU or GPU). learning_rate ( float ): Learning rate for the optimizer. batch_size ( int ): Number of samples processed per batch. n_epochs ( int ): Total number of training epochs. backbone_frozen ( bool ): Whether to freeze the backbone parameters during training. Returns losses ( tuple[float] ): A list of the average loss for each epoch during training. mAPs ( tuple[float] ): A list of the mean average precisions(mAP) on the validation set for each epoch during training. How It Works Loads the training dataset and splits it into training (90%) and validation (10%) sets. Initializes the Faster R-CNN model with the specified backbone and weights. Trains the model using the Adam optimizer and StepLR scheduler. Logs training progress (loss and mAP) for each epoch. Saves the best model (based on mAP) to a unique runXX folder. Example Usage from train import train from pprint import pprint losses, mAPs = train( image_dir=\"../final project/datasets/training_set/images\", label_dir=\"../final project/datasets/training_set/labels\", weights='Default', backbone='resnet34_fpn', learning_rate=1e-4, batch_size=32, n_epochs=2, backbone_frozen=False ) pprint(losses) pprint(mAPs) Python Output: Batch Size: 32 Weights: Default Backbone: resnet34_fpn Device: cuda Learning Rate: 0.0001 Number of Epochs: 2 Backbone Frozen: False Epoch [1/2] Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 107/107 [01:13<00:00, 1.46it/s] Validating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:04<00:00, 2.57it/s] Best model saved Loss: 0.557, mAP: 0.186 Epoch [2/2] Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 107/107 [01:13<00:00, 1.46it/s] Validating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:04<00:00, 2.50it/s] Best model saved Loss: 0.348, mAP: 0.370 (0.5567430853843689, 0.34804651141166687) (0.18615970015525818, 0.37041303515434265) The best model is saved in ./trains/run1/ and named best.pt . The log file is ./trains/run1/train.log .","title":"train.train"},{"location":"train/train/#traintrain","text":"train.train ( image_dir, label_dir, weights = 'Default' , backbone = 'resnet50_fpn' , device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") , learning_rate = 1e-4 , batch_size = 32 , n_epochs = 50 , backbone_frozen = False ) \u2192 tuple[tuple[float],tuple[float]] The function trains a Faster R-CNN model on a specified dataset and returns the training losses and validation mAP for each epoch. It creates a runXX folder under the trains directory, where XX is a number denoting the current training session's index. This folder contains the log file and the best model, selected based on the highest validation mAP score.","title":"train.train"},{"location":"train/train/#arguments","text":"image_dir ( str ): Path to the directory containing training images. label_dir ( str ): Path to the directory containing training labels in YOLO format. weights ( str ): Specifies how the model is initialized. Options: Pretrained : Uses pre-trained weights on our dataset. Recent : Uses the weights in the most recent run folder. Default : Uses original weights of Faster R-CNN. None : Starts with random weights. Custom Path: A .pt file containing saved weights. backbone ( str ): Backbone architecture. Options: resnet34_fpn resnet50_fpn mobilenetv3_fpn device ( torch.device ): Device to run training (CPU or GPU). learning_rate ( float ): Learning rate for the optimizer. batch_size ( int ): Number of samples processed per batch. n_epochs ( int ): Total number of training epochs. backbone_frozen ( bool ): Whether to freeze the backbone parameters during training.","title":"Arguments"},{"location":"train/train/#returns","text":"losses ( tuple[float] ): A list of the average loss for each epoch during training. mAPs ( tuple[float] ): A list of the mean average precisions(mAP) on the validation set for each epoch during training.","title":"Returns"},{"location":"train/train/#how-it-works","text":"Loads the training dataset and splits it into training (90%) and validation (10%) sets. Initializes the Faster R-CNN model with the specified backbone and weights. Trains the model using the Adam optimizer and StepLR scheduler. Logs training progress (loss and mAP) for each epoch. Saves the best model (based on mAP) to a unique runXX folder.","title":"How It Works"},{"location":"train/train/#example-usage","text":"from train import train from pprint import pprint losses, mAPs = train( image_dir=\"../final project/datasets/training_set/images\", label_dir=\"../final project/datasets/training_set/labels\", weights='Default', backbone='resnet34_fpn', learning_rate=1e-4, batch_size=32, n_epochs=2, backbone_frozen=False ) pprint(losses) pprint(mAPs) Python Output: Batch Size: 32 Weights: Default Backbone: resnet34_fpn Device: cuda Learning Rate: 0.0001 Number of Epochs: 2 Backbone Frozen: False Epoch [1/2] Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 107/107 [01:13<00:00, 1.46it/s] Validating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:04<00:00, 2.57it/s] Best model saved Loss: 0.557, mAP: 0.186 Epoch [2/2] Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 107/107 [01:13<00:00, 1.46it/s] Validating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:04<00:00, 2.50it/s] Best model saved Loss: 0.348, mAP: 0.370 (0.5567430853843689, 0.34804651141166687) (0.18615970015525818, 0.37041303515434265) The best model is saved in ./trains/run1/ and named best.pt . The log file is ./trains/run1/train.log .","title":"Example Usage"}]}